{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Named Entity Recognition Urdu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUszrANqxwAb",
        "colab_type": "text"
      },
      "source": [
        "**Named Entity Recognitin for Urdu**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8dKMZA6IZP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install the necessory packges for training \n",
        "pip install numpy keras matplotlib scikit-learn "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY11wx6ly-Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import all the necessory packeges \n",
        "import ast\n",
        "import codecs\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Dense, InputLayer, Embedding, Activation, LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q65MR473zJZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the dataset and train the model \n",
        "\n",
        "tagged_sentences = codecs.open('uner.txt', encoding=\"utf-8\").readlines()\n",
        "\n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "\n",
        "sentences, sentence_tags = [], []\n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tags = zip(*ast.literal_eval(tagged_sentence))\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        "\n",
        "(train_sentences,\n",
        " test_sentences,\n",
        " train_tags,\n",
        " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n",
        "\n",
        "words = get_words(sentences)\n",
        "tags = get_tags(sentence_tags)\n",
        "\n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0\n",
        "word2index['-OOV-'] = 1\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0\n",
        "\n",
        "train_sentences_x = get_train_sentences_x(train_sentences, word2index)\n",
        "test_sentences_x = get_test_sentences_x(test_sentences, word2index)\n",
        "\n",
        "train_tags_y = get_train_tags_y(train_tags, tag2index)\n",
        "test_tags_y = get_test_tags_y(test_tags, tag2index)\n",
        "\n",
        "MAX_LENGTH = len(max(train_sentences_x, key=len))\n",
        "# MAX_LENGTH = 181\n",
        "\n",
        "train_sentences_x = pad_sequences(train_sentences_x, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_x = pad_sequences(test_sentences_x, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH,)))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dense(len(tag2index)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_sentences_x, to_categorical(train_tags_y, len(tag2index)), batch_size=32, epochs=10,\n",
        "                    validation_split=0.2).history\n",
        "model.save(\"../models/lstm_ner.h5\")\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icwKv3iIMioZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test accuracy \n",
        "scores = model.evaluate(test_sentences_x, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")  # acc: 98.39311069478103\n",
        "print(test_sentences[0])\n",
        "print(test_tags[0])\n",
        "test_samples = [\n",
        "    test_sentences[0],\n",
        "]\n",
        "\n",
        "test_samples_x = []\n",
        "for sentence in test_samples:\n",
        "    sentence_index = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sentence_index.append(word2index[word])\n",
        "        except KeyError:\n",
        "            sentence_index.append(word2index['-OOV-'])\n",
        "    test_samples_x.append(sentence_index)\n",
        "\n",
        "test_samples_X = pad_sequences(test_samples_x, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "predictions = model.predict(test_samples_X)\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewhSLIEMMlJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot model\n",
        "plt.plot(history['acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='lower right')\n",
        "plt.savefig(\"../results/accuracy_lstm_ner.png\")\n",
        "plt.clf()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper right')\n",
        "plt.savefig(\"../results/loss_lstm_ner.png\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}